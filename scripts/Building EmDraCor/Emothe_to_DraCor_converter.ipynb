{"cells":[{"cell_type":"markdown","source":["# Emothe-to-DraCor Conversion Script\n","\n","Python script used to convert 150+ texts found in the [Biblioteca Digital Artelope](https://emothe.uv.es/biblioteca/) ([Emothe project](https://emothe.uv.es/), University of Valencia) to a DraCor-ready .xml format.\n","\n","Written by [Daniil Skorinkin](https://github.com/DanilSko) (2022)."],"metadata":{"id":"iPbu6pBwkjQf"},"id":"iPbu6pBwkjQf"},{"cell_type":"markdown","id":"4207c452","metadata":{"id":"4207c452"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"id":"595f99eb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"595f99eb","executionInfo":{"status":"ok","timestamp":1668771246913,"user_tz":-60,"elapsed":5382,"user":{"displayName":"Luca Giovannini","userId":"09190813829789814211"}},"outputId":"a157c242-d1c7-4012-85f4-c6238010a2d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=83f8e22cd7eb65e99be571b4d5c1ca5e9f532b38078c77a81b604e01ac6ad0b5\n","  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"]}],"source":["from bs4 import BeautifulSoup, Tag\n","import re\n","import os\n","!pip install wget\n","import wget\n","import time\n","import requests\n","from tqdm import tqdm\n","from datetime import datetime"]},{"cell_type":"code","execution_count":null,"id":"46ae3be2","metadata":{"id":"46ae3be2"},"outputs":[],"source":["#soup.findAll('div',type='elenco')"]},{"cell_type":"markdown","id":"7da7b5bf","metadata":{"id":"7da7b5bf"},"source":["## Reading data"]},{"cell_type":"code","execution_count":null,"id":"b8c84745","metadata":{"id":"b8c84745"},"outputs":[],"source":["xmls_addresses = ['https://emothe.uv.es/biblioteca/textosXML/EMOTHE0361_ElCaballeroDeOlmedo.xml']"]},{"cell_type":"code","execution_count":null,"id":"43430bb7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43430bb7","executionInfo":{"status":"ok","timestamp":1668771246926,"user_tz":-60,"elapsed":104,"user":{"displayName":"Luca Giovannini","userId":"09190813829789814211"}},"outputId":"ac9acdb6-205b-4da9-e414-51c32516086e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":4}],"source":["len(xmls_addresses)"]},{"cell_type":"code","execution_count":null,"id":"cbaf387e","metadata":{"id":"cbaf387e"},"outputs":[],"source":["!mkdir 'source_xmls'"]},{"cell_type":"code","execution_count":null,"id":"5f095819","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5f095819","executionInfo":{"status":"ok","timestamp":1668771247285,"user_tz":-60,"elapsed":417,"user":{"displayName":"Luca Giovannini","userId":"09190813829789814211"}},"outputId":"ed648b5f-0a9e-4669-df4b-53e1facf8ddb"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n","  InsecureRequestWarning)\n","100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n"]}],"source":["for url in tqdm(xmls_addresses):\n","    filename = url.split('/')[-1]\n","    time.sleep(0.1)\n","    filetext = requests.get(url, verify=False).text\n","    with open(f'source_xmls/{filename}','w') as filetowrite:\n","        filetowrite.write(filetext)"]},{"cell_type":"markdown","id":"e8dd6e3c","metadata":{"id":"e8dd6e3c"},"source":["## Transformation of files"]},{"cell_type":"markdown","id":"1b251e09","metadata":{"id":"1b251e09"},"source":["### Transformation functions"]},{"cell_type":"code","execution_count":null,"id":"2e3e06a0","metadata":{"id":"2e3e06a0"},"outputs":[],"source":["def add_partic_desc(soup):\n","    set_of_char_pairs = set() # множество пар ID + строка \n","    for sp in soup.find_all('sp'):\n","        try:\n","          if 'who' in sp.attrs:\n","              set_of_char_pairs.add((sp['who'], sp.speaker.text.strip('.: '))) \n","        except:\n","          pass\n","    add_particdesc_to_header(soup, set_of_char_pairs)"]},{"cell_type":"code","execution_count":null,"id":"f214dfc5","metadata":{"id":"f214dfc5"},"outputs":[],"source":["def guess_gender(persName):\n","    if persName.lower().strip('.:[]()').endswith('a'):\n","        return 'FEMALE'\n","    return 'MALE'"]},{"cell_type":"code","execution_count":null,"id":"5623eed5","metadata":{"id":"5623eed5"},"outputs":[],"source":["def add_particdesc_to_header(soup, set_of_char_pairs):\n","    #print(set_of_char_pairs)\n","    profileDesc = soup.find('profileDesc')\n","    if profileDesc is None:\n","        profileDesc = Tag(name = 'profileDesc') \n","    particDesc = Tag(name = 'particDesc')\n","    profileDesc.append(particDesc)\n","    listPerson = Tag(name = 'listPerson')\n","    particDesc.append(listPerson)\n","    for pair in set_of_char_pairs:\n","        person = Tag(name = 'person')\n","        person['xml:id'] = pair[0].strip('#')\n","        persName = Tag(name = 'persName')\n","        person.append(persName)\n","        person['sex'] = guess_gender(pair[1]) \n","        #print(pair[1])\n","        persName.append(pair[1])\n","        listPerson.append(person)\n","    teiHeader = soup.find('teiHeader')\n","    teiHeader.append(profileDesc)"]},{"cell_type":"code","execution_count":null,"id":"4e573ee1","metadata":{"id":"4e573ee1"},"outputs":[],"source":["tags_to_remove = ['sponsor', 'funder', 'appInfo', \n","                  'respStmt', 'principal', 'notesStmt', \n","                  'encodingDesc', 'langUsage', 'pubPlace',\n","                  'extent', 'authority', 'editionStmt', 'revisionDesc'\n","                 ]\n","\n","tags_to_check_parent_and_remove = {'date':'publicationStmt'}\n","tags_with_attrs_to_remove = {'author':['key'], 'stage':['xml:id'], 'editor':['role'], \n","                             'l':['xml:id', 'n'], 'sp':['xml:id'], 'div1':['xml:id', 'n'],\n","                             'div':['xml:id', 'n'], 'div2':['xml:id', 'n'],\n","                             'role':['xml:id']}\n","\n","attrs_to_be_renamed = [\n","    {'tag':'div', 'attr':'type', \n","     'old_value':'elenco', \n","     'new_value':'Dramatis_Personae'},\n","    {'tag':'div', 'attr':'type', \n","     'old_value':'elenco', \n","     'new_value':'Dramatis_Personae'}\n","]\n","tags_to_rename_and_add_attr = {'div1':('div', {'type':'act'}), 'div2':('div', {'type':'scene'})}\n","tags_to_check_attr_and_remove = [\n","    {'tag':'author', 'attr':'ana', \n","     'value':'fiable'},\n","    {'tag':'author', 'attr':'key', \n","     'value':'archivo'},\n","    {'tag':'title', 'attr':'key', \n","     'value':'archivo'},\n","    {'tag':'title', 'attr':'key', \n","     'value':'orden'}\n","] \n","\n","tags_to_replace_with_children = ['lg']"]},{"cell_type":"code","execution_count":null,"id":"f3e4a027","metadata":{"id":"f3e4a027"},"outputs":[],"source":["def remove_tags(soup, tags_to_remove):\n","    for tag in tags_to_remove:\n","        all_occurrences = soup.findAll(tag)\n","        for tag_instance in all_occurrences:\n","            tag_instance.decompose()"]},{"cell_type":"code","execution_count":null,"id":"d6205ebe","metadata":{"id":"d6205ebe"},"outputs":[],"source":["def remove_tags_parent_check(soup, tags_to_check_parent_and_remove):\n","    for tag in tags_to_check_parent_and_remove:\n","        all_occurrences = soup.findAll(tag)\n","        parent = tags_to_check_parent_and_remove[tag]\n","        for tag_instance in all_occurrences:\n","            if tag_instance.parent.name == parent:\n","                tag_instance.decompose()"]},{"cell_type":"code","execution_count":null,"id":"a1ce0cb1","metadata":{"id":"a1ce0cb1"},"outputs":[],"source":["def rename_attrs(soup, attrs_to_be_renamed):\n","    for attr in attrs_to_be_renamed:\n","        tagname = attr['tag']\n","        attrname = attr['attr']\n","        old_value = attr['old_value']\n","        new_value = attr['new_value']\n","        all_occurrences = soup.findAll(tagname, attrs={attrname: old_value})\n","        for tag_instance in all_occurrences: \n","            tag_instance[attrname] = new_value"]},{"cell_type":"code","execution_count":null,"id":"de3f5b0d","metadata":{"id":"de3f5b0d"},"outputs":[],"source":["def remove_attrs(soup, tags_with_attrs_to_remove):\n","    for tag in tags_with_attrs_to_remove:\n","        all_occurrences = soup.findAll(tag)\n","        attrs = tags_with_attrs_to_remove[tag]\n","        for tag_instance in all_occurrences:\n","            for attr in attrs:\n","                del tag_instance[attr]"]},{"cell_type":"code","execution_count":null,"id":"9c7093a9","metadata":{"id":"9c7093a9"},"outputs":[],"source":["def check_attr_and_remove(soup, tags_to_check_attr_and_remove):\n","    for tag in tags_to_check_attr_and_remove:\n","        tagname = tag['tag']\n","        attrname = tag['attr']\n","        old_value = tag['value']\n","        all_occurrences = soup.findAll(tagname, attrs={attrname: old_value})\n","        for tag_instance in all_occurrences: \n","            tag_instance.decompose()"]},{"cell_type":"code","execution_count":null,"id":"a33b60c5","metadata":{"id":"a33b60c5"},"outputs":[],"source":["def rename_tags_and_add_attr(soup, tags_to_rename_and_add_attr):\n","    for tag in tags_to_rename_and_add_attr:\n","        tagtofind = tag\n","        newname = tags_to_rename_and_add_attr[tag][0]\n","        newattrs = tags_to_rename_and_add_attr[tag][1]\n","        all_occurrences = soup.findAll(tagtofind)\n","        for tag_instance in all_occurrences: \n","            tag_instance.name = newname\n","            for attr in newattrs:\n","                value = newattrs[attr]\n","                tag_instance[attr] = value"]},{"cell_type":"code","execution_count":null,"id":"c111534e","metadata":{"id":"c111534e"},"outputs":[],"source":["def replace_with_children(soup, tags_to_replace_with_children):\n","    for tag in tags_to_replace_with_children:\n","        all_occurrences = soup.findAll(tag)\n","        for tag_instance in all_occurrences: \n","            tag_instance.replaceWithChildren()"]},{"cell_type":"code","execution_count":null,"id":"33ec7f4f","metadata":{"id":"33ec7f4f"},"outputs":[],"source":["def add_standoff(soup):\n","    #today = datetime.today().strftime('%Y')\n","    standoff_as_string = f'''\n","    <standOff>\n","        <listEvent>\n","        <event type=\"print\" when=\"9999\">\n","        <desc/>\n","        </event>\n","        <event type=\"premiere\" when=\"9999\">\n","        <desc/>\n","        </event>\n","        <event type=\"written\" when=\"9999\">\n","        <desc/>\n","        </event>\n","        </listEvent>\n","        <listRelation>\n","        <relation name=\"wikidata\" active=\"INSERT\" passive=\"INSERT\"/>\n","        </listRelation>\n","    </standOff>\n","    '''\n","    standoffsoup = BeautifulSoup(standoff_as_string, 'xml')\n","    standoff = standoffsoup.standOff\n","    soup.teiHeader.insert_after(standoff)"]},{"cell_type":"code","execution_count":null,"id":"969452f0","metadata":{"id":"969452f0"},"outputs":[],"source":["def replace_pbstmt(soup):\n","    try:\n","        soup.find('publicationStmt').decompose()\n","        pubstmt_as_string = \"\"\"\n","          <publicationStmt>\n","            <publisher xml:id=\"dracor\">DraCor</publisher>\n","            <idno type=\"URL\">https://dracor.org</idno>\n","            <availability>\n","              <licence>\n","                <ab>CC0 1.0</ab>\n","                <ref target=\"https://creativecommons.org/publicdomain/zero/1.0/\">Licence</ref>\n","              </licence>\n","            </availability>\n","          </publicationStmt>\n","        \"\"\"\n","        pbsoup = BeautifulSoup(pubstmt_as_string, 'xml')\n","        pbstmt = pbsoup.publicationStmt\n","        soup.titleStmt.insert_after(pbstmt)\n","    except:\n","        pass"]},{"cell_type":"code","execution_count":null,"id":"00a8ea33","metadata":{"id":"00a8ea33"},"outputs":[],"source":["def replace_textclass(soup):\n","    soup.find('textClass').decompose()\n","    textClass_as_string = \"\"\"\n","      <textClass>\n","        <keywords>\n","          <term type=\"genreTitle\">insert genre</term>\n","        </keywords>\n","        <classCode scheme=\"http://www.wikidata.org/entity/\">insert wikidata code</classCode>\n","      </textClass>\n","    \"\"\"\n","    txtclasssoup = BeautifulSoup(textClass_as_string, 'xml')\n","    txtclasstag = txtclasssoup.textClass\n","    soup.particDesc.insert_after(txtclasstag)"]},{"cell_type":"code","execution_count":null,"id":"3674a087","metadata":{"id":"3674a087"},"outputs":[],"source":["def replace_titlepage_with_head(soup):\n","    try:\n","        titlePage = soup.find('titlePage')\n","        titletext = titlePage.find('title').text\n","        head = soup.new_tag('head')\n","        head.append(titletext)\n","        titlePage.insert_after(head)\n","        titlePage.decompose()\n","    except:\n","        pass"]},{"cell_type":"code","execution_count":null,"id":"5bf39c86","metadata":{"id":"5bf39c86"},"outputs":[],"source":["def split_author_name(soup):\n","    author = soup.find('author')\n","    text = author.text\n","    splitname = text.split(',')\n","    if len(splitname) == 2:\n","        author.clear()\n","        forename = soup.new_tag('forename')\n","        forename.append(splitname[0].strip())\n","        author.append(forename)\n","        surname = soup.new_tag('surname')\n","        surname.append(splitname[1].strip())\n","        author.append(surname)"]},{"cell_type":"markdown","id":"59cbcb10","metadata":{"id":"59cbcb10"},"source":["### Applying transformation"]},{"cell_type":"code","execution_count":null,"id":"3d580acd","metadata":{"id":"3d580acd"},"outputs":[],"source":["!mkdir 'transformed'"]},{"cell_type":"code","execution_count":null,"id":"3b687e5f","metadata":{"id":"3b687e5f"},"outputs":[],"source":["def process_file(path_to_file):\n","    with open(path_to_file, 'r') as openfile: #, encoding='utf-16'\n","        file_as_text = openfile.read()\n","    #    try:\n","        soup = BeautifulSoup(file_as_text, 'xml')\n","        add_partic_desc(soup)\n","        remove_tags(soup, tags_to_remove)\n","        remove_tags_parent_check(soup, tags_to_check_parent_and_remove)\n","        rename_attrs(soup, attrs_to_be_renamed)\n","        remove_attrs(soup, tags_with_attrs_to_remove)\n","        rename_tags_and_add_attr(soup, tags_to_rename_and_add_attr)\n","        replace_pbstmt(soup)\n","        add_standoff(soup)\n","        replace_textclass(soup)\n","        check_attr_and_remove(soup, tags_to_check_attr_and_remove)\n","        replace_with_children(soup, tags_to_replace_with_children)\n","        replace_titlepage_with_head(soup)\n","        split_author_name(soup)\n","        new_path = path_to_file.replace('.xml', '_transformed.xml')\n","        new_path = new_path.replace('source_xmls', 'transformed')\n","        with open(new_path, 'w') as output:\n","            output.write(str(soup))\n","     #   except:\n","      #      print(f'failed to process' , path_to_file)"]},{"cell_type":"code","execution_count":null,"id":"46b4d07a","metadata":{"id":"46b4d07a"},"outputs":[],"source":["#test on one:\n","#process_file('source_xmls/EMOTHE0383_LosAmantes.xml')"]},{"cell_type":"code","execution_count":null,"id":"05ada7f8","metadata":{"id":"05ada7f8"},"outputs":[],"source":["#process all:\n","for xmlfilename in os.listdir('source_xmls'):\n","    if '.xml' in xmlfilename:\n","        process_file(os.path.join('source_xmls', xmlfilename))"]},{"cell_type":"markdown","id":"1d98c7e6","metadata":{"id":"1d98c7e6"},"source":["### Adding indents"]},{"cell_type":"code","source":["#upload the xml formatter\n","\n","if not os.path.isfile('format.conf'):\n","    wget.download('https://raw.githubusercontent.com/lucagiovannini7/baroque-networks/main/dracor-xmls/xmlformat.pl')\n","\n","!cp xmlformat.pl /usr/local/bin/xmlformat\n","\n","!chmod 755 -R /usr/local/bin/xmlformat\n"],"metadata":{"id":"SQ278JAJV1m_"},"id":"SQ278JAJV1m_","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"2d42dab0","metadata":{"id":"2d42dab0"},"outputs":[],"source":["#upload the format.conf file\n","\n","if not os.path.isfile('format.conf'):\n","    wget.download('https://raw.githubusercontent.com/dracor-org/gerdracor/main/format.conf')"]},{"cell_type":"code","execution_count":null,"id":"165ebc6d","metadata":{"id":"165ebc6d"},"outputs":[],"source":["for filename in os.listdir('transformed'):\n","    if '_transformed.xml' in filename:\n","        file_path = os.path.join('transformed', filename)\n","        try:\n","            os.system(f'xmlformat --config-file=format.conf \\\"{file_path}\\\" > \\\"{file_path.replace(\".xml\",\"\")}_indented.xml\\\" ')\n","            os.system(f'rm {file_path}')\n","        except:\n","            print('indentation failed:', filename)"]},{"cell_type":"code","execution_count":null,"id":"b4674896","metadata":{"id":"b4674896"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[{"file_id":"1OYATIahr0p0pp2G_CgKHxwh0gXpp-9AM","timestamp":1668534597796}]}},"nbformat":4,"nbformat_minor":5}
